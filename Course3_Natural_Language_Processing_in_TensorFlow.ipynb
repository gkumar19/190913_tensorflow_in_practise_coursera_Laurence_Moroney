{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course3: Natural Language Processing in TensorFlow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDqUGXIeCDy2",
        "colab_type": "text"
      },
      "source": [
        "Week1: Using Text Tokenizer\n",
        "Note: Things are in list rather than array here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkgcplIOB-XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sentences = ['i love you',\n",
        "             'i need you',\n",
        "             'dog is the best of all',\n",
        "             'i am sleeping']\n",
        "test_sentences = ['Jaadu need you',\n",
        "                  'i dont love darkness'\n",
        "                  'lets play holi',\n",
        "                  'god is there with all of us']\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 9, oov_token = '<00v>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "print(tokenizer.index_word)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(sequences)\n",
        "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences , maxlen = 5 ,padding = 'post', truncating = 'pre')\n",
        "print(padded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX1Kobxy1rfx",
        "colab_type": "text"
      },
      "source": [
        "Week2: Tensorflow Datasets, embedding layer, getting weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4NksqZG1uk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "VOCABSIZE = 10000\n",
        "MAXLEN = 120\n",
        "EMBDIM = 16\n",
        "EMBMAXLEN = 120\n",
        "\n",
        "print(tf.__version__)\n",
        "imdb,info = tfds.load(\"imdb_reviews\",with_info= True, as_supervised= True, download=False)\n",
        "\n",
        "train_data, test_data = imdb['train'] , imdb['test']\n",
        "\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "test_sentences = []\n",
        "test_labels = []\n",
        "\n",
        "for feature,label in train_data:\n",
        "    train_sentences.append(str(feature.numpy()))\n",
        "    train_labels.append(label.numpy())\n",
        "\n",
        "for feature,label in test_data:\n",
        "    test_sentences.append(str(feature.numpy()))\n",
        "    test_labels.append(label.numpy())\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='',num_words = VOCABSIZE)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences,padding = 'pre', maxlen = MAXLEN,truncating = 'pre')\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences,padding = 'pre', maxlen = MAXLEN,truncating = 'pre')\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM,input_length=EMBMAXLEN), #Embedding takes Senteces, as a matrix of EMBMAXLEN x VOCABSIZE [one-hot-rep] and multiplies with embedding matrix VOCABSIZE x EMBDIM and returns a matrix representing the sentence in embedding feature space of size EMBMAXLEN x EMBDIM\n",
        "                                    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'),\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer= tf.keras.optimizers.Adam(lr = 0.001), metrics = ['accuracy'] )\n",
        "history = model.fit(train_padded,train_labels,\n",
        "          validation_data = (test_padded,test_labels),\n",
        "          epochs=50)\n",
        "\n",
        "#Check the weights of layers\n",
        "emb_weights = model.get_weights()[0]\n",
        "emb_weights2 = model.layers[0].get_weights()[0]\n",
        "\n",
        "#Plotting the learning curves\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure('losses')\n",
        "plt.plot(history.history['loss'], label = 'train loss')\n",
        "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
        "\n",
        "plt.figure('accuracy')\n",
        "plt.plot(history.history['acc'], label = 'train accuracy')\n",
        "plt.plot(history.history['val_acc'], label = 'validation accuracy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTbblpO8xrvT",
        "colab_type": "text"
      },
      "source": [
        "Week3: LSTM , GRU , Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uop4sbDJ1PKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Modify the Week2 code as per:\n",
        "VOCABSIZE = 10000\n",
        "MAXLEN = 120\n",
        "EMBDIM = 16\n",
        "EMBMAXLEN = 120\n",
        "\n",
        "#Original Model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM,input_length=EMBMAXLEN), #None,120,16\n",
        "                                    tf.keras.layers.GlobalAveragePooling1D(), #Output_shape = None,16\n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'), #None,6\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')]) # None,1\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM, input_length=EMBMAXLEN), #None,120,16\n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), #None,128 , parameters = 31104\n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'), #None,6\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')]) #None,1\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM, input_length=EMBMAXLEN), #(None, 120, 16) \n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), #(None, 120, 128)\n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)), #(None, 128) \n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'), #(None, 6)\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')]) #(None, 1)\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM, input_length=EMBMAXLEN),#(None, 120, 16) \n",
        "                                    tf.keras.layers.Conv1D(51, kernel_size=5,activation = 'relu'),#(None, 116, 51)  \n",
        "                                    tf.keras.layers.GlobalAveragePooling1D(), #(None, 51)\n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'), #(None, 6) \n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')]) #(None, 1)\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Embedding(VOCABSIZE,EMBDIM, input_length=EMBMAXLEN), #(None, 120, 16)\n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),#(None, 128) , parameters = 41472\n",
        "                                    tf.keras.layers.Dense(6,activation = 'relu'),#(None, 6)\n",
        "                                    tf.keras.layers.Dense(1, activation = 'sigmoid')]) #(None, 1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}